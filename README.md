# ðŸ’¼ Azure Data Pipeline â€“ Retail Customer Transactions

This project showcases an **Azure Data Factory pipeline** that performs **event-triggered ingestion**, **row-level data validation**, **SQL database loading**, and **error handling** using cloud-native services.

---

## ðŸ“Œ Project Summary

A CSV file containing customer transactions is uploaded to **Azure Blob Storage**, triggering an ADF pipeline. The pipeline uses a **Mapping Data Flow** to validate rows, send valid records to **Azure SQL Database**, send invalid records to **Blob error folder**, and archive the source file.

---

## ðŸš€ Key Features

- âœ… Event-based trigger when `.csv` is uploaded
- ðŸ” Row-level validation using ADF **Conditional Split**:
  - Email must not be null
  - Amount must be > 0
  - Created date must be present
- ðŸ“¥ Valid data â†’ stored in `Azure SQL`
- ðŸ“¤ Invalid data â†’ stored in `error-data/` folder with timestamped file name
- ðŸ“¦ Source file â†’ archived in `processed-data/`

---

## ðŸ§± Tech Stack

| Component              | Usage                            |
|------------------------|----------------------------------|
| Azure Data Factory     | Orchestration & Data Flow        |
| Azure Blob Storage     | Source, Error, Archive storage   |
| Azure SQL Database     | Final staging table              |
| Azure Event Grid       | Trigger pipeline on blob upload  |
| Data Flow Parameters   | Used for dynamic file naming     |

---

## ðŸ“ Folder Structure
azure-data-pipeline-retail-validation/
â”œâ”€â”€ data/ # Sample CSV files
â”œâ”€â”€ sql/ # SQL script to create table
â”œâ”€â”€ pipeline-screenshots/ # ADF screenshots
â”œâ”€â”€ adf_export/ # (Optional) JSON/ARM export
â”œâ”€â”€ README.md


---

## ðŸ“„ SQL Table (Target Schema)

```sql
CREATE TABLE stg_customer_transactions (
  customer_id VARCHAR(20),
  email VARCHAR(100),
  amount FLOAT,
  created_date DATE
);

ðŸ“‚ Sample File Paths
Type	Path
Input CSV	raw-data/customer_transactions.csv
Valid Output	Azure SQL Table: stg_customer_transactions
Invalid Output	error-data/invalid_rows_YYYYMMDD_HHMMSS.csv
Archived File	processed-data/customer_transactions_YYYYMMDD.csv

ðŸ§ª Sample Data

customer_id,email,amount,created_date
C101,neha.sharma@mail.com,1250,2024-06-01
C102,arjun.rao@mail.com,900,2024-06-02
C103,,0,2024-06-03
C104,,300,2024-06-04
C105,rahul.patel@mail.com,750,


ðŸ›  How It Works
Upload CSV to raw-data container

Event trigger starts ADF pipeline

Data Flow:

Valid rows go to SinkSQL

Invalid rows go to SinkErrorBlob

File archived to processed-data/

Monitor pipeline under Monitor > Pipeline Runs


ðŸ”„ Possible Enhancements

Add schema drift handling

Parameterize container names and paths

Add logging or email notifications

Build Power BI dashboard on top of SQL data

ðŸ‘¤ Author
Priya Banerjee
Data Engineer | Azure | Power BI | SQL

